# -*- coding: utf-8 -*-
"""Model-Evaluation and Topsis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VXHJuMdAKvnZ3PgBxBYb10oR2FO9abvM
"""

!pip install nltk
!pip install transformers
import nltk
# Download the WordNet resource
nltk.download('wordnet')

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from transformers import GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification, XLNetTokenizer, XLNetForSequenceClassification
import torch
import json
import numpy as np
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def evaluate_model(model, input_text, reference, tokenizer):
  # Load GPT-2 tokenizer and model
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")
    generated = model.generate(tokenizer.encode(input_text, return_tensors="pt"), max_length=50, num_beams=5)
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)

    # Use BLEU score for fluency evaluation
    fluency_score = sentence_bleu([reference.split()], generated_text.split(), smoothing_function=SmoothingFunction().method7)

    # Use simple matching for accuracy evaluation
    accuracy_score = int(reference == generated_text)

    # Coherence score (already present in the third column)
    coherence_score = float(0.25)  # Placeholder

    # Relevance score (currently set to zero)
    relevance_score = float(0.25)  # Placeholder

    return [accuracy_score, fluency_score, coherence_score, relevance_score]

# Load your conversation dataset from JSON file
with open('/content/text_conv.json', 'r') as file:
    conversation_data = json.load(file)

# Evaluate pretrained models
models = {
    "gpt2": {"model": GPT2LMHeadModel.from_pretrained("gpt2"), "tokenizer": GPT2Tokenizer.from_pretrained("gpt2")},
    "t5": {"model": T5ForConditionalGeneration.from_pretrained("t5-small"), "tokenizer": T5Tokenizer.from_pretrained("t5-small")},
    "bert": {"model": BertForSequenceClassification.from_pretrained("bert-base-uncased"), "tokenizer": BertTokenizer.from_pretrained("bert-base-uncased")},
    "distilbert": {"model": DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased"), "tokenizer": DistilBertTokenizer.from_pretrained("distilbert-base-uncased")},
    "xlnet": {"model": XLNetForSequenceClassification.from_pretrained("xlnet-base-cased"), "tokenizer": XLNetTokenizer.from_pretrained("xlnet-base-cased")}
}

model_scores = {model_name: [] for model_name in models}

# Assuming your dataset has a list of messages with 'content' field
for i in range(1, len(conversation_data['messages'])):
    input_text = conversation_data['messages'][i-1]['content']
    reference = conversation_data['messages'][i]['content']

    for model_name, model_info in models.items():
        scores = evaluate_model(model_info["model"], input_text, reference, model_info["tokenizer"])
        model_scores[model_name].append(scores)

# Convert the lists to NumPy arrays for TOPSIS
import numpy as np
from prettytable import PrettyTable


# Convert the lists to NumPy arrays for TOPSIS
model_scores = {model_name: np.array(scores) for model_name, scores in model_scores.items()}
weights = np.array([0.25, 0.25, 0.25, 0.25, 0.25])  # Add a weight for training time

# Check if there are any NaN or inf values in model scores
for model_name, scores in model_scores.items():
    if np.isnan(scores).any() or np.isinf(scores).any():
        print(f"{model_name} scores contain NaN or inf values. Please check your evaluation function.")
    else:
        # Normalize the model scores with a small epsilon to avoid division by zero
        epsilon = 1e-10
        normalized_scores = scores / (np.linalg.norm(scores, axis=0) + epsilon)

        # Include training time in normalized scores
        normalized_scores[:, -1] *= weights[-1]

       # Define the ideal and non-ideal solutions
        ideal_solution = np.array([[1, 1, 1, 1]] * len(normalized_scores))  # Assuming higher values are better for all metrics
        non_ideal_solution = np.array([[0, 0, 0, 0]] * len(normalized_scores))  # Assuming lower values are better for all metrics

        # Calculate distances
        distance_to_ideal = np.linalg.norm(normalized_scores - ideal_solution, axis=1)
        distance_to_non_ideal = np.linalg.norm(normalized_scores - non_ideal_solution, axis=1)

        # Calculate TOPSIS score
        topsis_score = distance_to_non_ideal / (distance_to_ideal + distance_to_non_ideal)

        # Ranking
        ranking = np.argsort(topsis_score)[::-1]

        # Print the results
        print(f"\nResults for {model_name}:")
        for i, rank in enumerate(ranking):
            print(f"Rank {i + 1}: Message {rank}, TOPSIS Score: {topsis_score[rank]}")